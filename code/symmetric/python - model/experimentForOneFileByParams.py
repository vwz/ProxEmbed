#encoding=utf-8
'''
Training one dataset and then test NDCG and MAP.
'''

import numpy
import theano
from theano import tensor

import proxEmbed
import proxEmbedProcessAndAssess
import os

import ConfigParser
import string, os, sys


if __name__=='__main__':
    
    cf = ConfigParser.SafeConfigParser()
    # read the parameters file.
#     cf.read("/usr/lzmExperiment/proxEmbed/paramsSet/pythonParamsConfig")
    cf.read("pythonParamsConfig")
    
    main_dir=cf.get("param", "root_dir") # main work dir
    dataset_name=cf.get("param", "dataset_name") # the name of one dataset
    suffix=cf.get("param", "suffix") # the suffix of dataset, such as 10,100,1000
    class_name=cf.get("param", "class_name") # the relation name of data
    index=cf.get("param", "index") # the index of the dataset file
    
    trainingDataFile=os.path.join(main_dir+'/',dataset_name+'.splits','train.'+suffix,'train_'+class_name+'_'+index) # the full path of training data file. This path will be generated by main_dir, dataset_name, suffix, class_name and index.
    wordsEmbeddings=None # words embeddings
    wordsEmbeddings_path=cf.get("param", "wordsEmbeddings_path") # the file path of words embeddings
    word_dimension=cf.getint("param", "word_dimension") # dimension of words embeddings
    dimension=cf.getint("param", "dimension") # the dimension of paths embeddings
    wordsSize=cf.getint("param", "wordsSize") # the size of words vocabulary
    subpaths_map=None # contains sub-paths
    subpaths_file=cf.get("param", "subpaths_file") # the file which contains sub-paths
    maxlen_subpaths=cf.getint("param", "maxlen_subpaths") # the max length for sub-paths
    h_output_method=cf.get("param", "h_output_method") # the output way of lstm. There are three ways, "h" only uses the last output h as the output of lstm for one path; "mean-pooling" uses the mean-pooling of all hi as the output of lstm for one path; "max-pooling" uses the max-pooling of all hi as the output of lstm for one path.
    maxlen=cf.getint("param", "maxlen")  # Sequence longer than this get ignored 
    batch_size=cf.getint("param", "batch_size") # use a batch for training. This is the size of this batch.
    is_shuffle_for_batch=cf.getboolean("param", "is_shuffle_for_batch") # if need shuffle for training
    discount_alpha=cf.getfloat("param", "discount_alpha") # the parameter alpha for discount. The longer the subpath, the little will the weight be.
    subpaths_pooling_method=cf.get("param", "subpaths_pooling_method") # the ways to combine several subpaths to one. "mean-pooling" means to combine all subpaths to one by mean-pooling; "max-pooling" means to combine all subpaths to one by max-pooling.
    objective_function_method=cf.get("param", "objective_function_method") # loss function, we use sigmoid
    objective_function_param=cf.getfloat("param", "objective_function_param") # the parameter in loss function, beta
    lrate=cf.getfloat("param", "lrate") # learning rate
    max_epochs=cf.getint("param", "max_epochs") # the max epochs for training
    
    dispFreq=cf.getint("param", "dispFreq")  # the frequences for display
    saveFreq=cf.getint("param", "saveFreq") # the frequences for saving the parameters
    saveto=os.path.join(main_dir+'/',dataset_name+'.trainModels','train.'+suffix,'train_'+class_name+'_'+index+'.npz') # the path for saving parameters. It is generated by main_dir, dataset_name, suffix, class_name and index.
    
    # the normalization of this model, l2-norm of all parameters
    decay_lstm_W=cf.getfloat("param", "decay_lstm_W") 
    decay_lstm_U=cf.getfloat("param", "decay_lstm_U") 
    decay_lstm_b=cf.getfloat("param", "decay_lstm_b") 
    decay_w=cf.getfloat("param", "decay_w") 
    
    test_data_file=os.path.join(main_dir+'/',dataset_name+'.splits','test','test_'+class_name+'_'+index) # the file of test data
    top_num=cf.getint("param", "top_num") # the top num to predict
    ideal_data_file=os.path.join(main_dir+'/',dataset_name+'.splits','ideal','ideal_'+class_name+'_'+index) # the file of ground truth
    
    # training
    proxEmbed.proxEmbedTraining(
                     trainingDataFile, 
                     wordsEmbeddings, 
                     wordsEmbeddings_path, 
                     word_dimension, 
                     dimension,
                     wordsSize, 
                     subpaths_map, 
                     subpaths_file,
                     maxlen_subpaths, 
                     h_output_method, 
                     maxlen,  
                     batch_size, 
                     is_shuffle_for_batch, 
                     discount_alpha,
                     subpaths_pooling_method, 
                     objective_function_method, 
                     objective_function_param, 
                     lrate, 
                     max_epochs, 
                      
                     dispFreq, 
                     saveFreq, 
                     saveto, 
                      
                     decay_lstm_W, 
                     decay_lstm_U, 
                     decay_lstm_b, 
                     decay_w, 
                     )
    
    # load the function which is trained beforehand
    computeFunc=proxEmbedProcessAndAssess.get_proxEmbedModel(
                     saveto, 
                     word_dimension, 
                     dimension, 
                     h_output_method, 
                     discount_alpha, 
                     subpaths_pooling_method, 
                      )
    # test the model
    MAP,MnDCG=proxEmbedProcessAndAssess.compute_proxEmbed(
                     wordsEmbeddings, 
                     wordsEmbeddings_path, 
                     word_dimension, 
                     dimension, 
                     wordsSize, 
                     subpaths_map, 
                     subpaths_file,
                     maxlen_subpaths, 
                     maxlen,  
                     
                     test_data_file,
                     top_num, 
                     ideal_data_file, 
                     func=computeFunc, 
                   )
    
    print 'MAP==',MAP
    print 'MnDCG==',MnDCG